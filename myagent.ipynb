{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='accounts/fireworks/models/llama-guard-3-11b-vision', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-guard-3-11b-vision', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-guard-3-8b', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-guard-3-8b', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p1-405b-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-405b-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p1-70b-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-70b-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p1-8b-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-8b-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p2-11b-vision-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-11b-vision-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p2-3b-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-3b-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p2-90b-vision-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-90b-vision-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama-v3p3-70b-instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p3-70b-instruct', model_type='llm'), Model(identifier='accounts/fireworks/models/llama4-maverick-instruct-basic', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama4-maverick-instruct-basic', model_type='llm'), Model(identifier='accounts/fireworks/models/llama4-scout-instruct-basic', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama4-scout-instruct-basic', model_type='llm'), Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='all-MiniLM-L6-v2', model_type='embedding'), Model(identifier='anthropic/claude-3-5-haiku-latest', metadata={}, api_model_type='llm', provider_id='anthropic', type='model', provider_resource_id='anthropic/claude-3-5-haiku-latest', model_type='llm'), Model(identifier='anthropic/claude-3-5-sonnet-latest', metadata={}, api_model_type='llm', provider_id='anthropic', type='model', provider_resource_id='anthropic/claude-3-5-sonnet-latest', model_type='llm'), Model(identifier='anthropic/claude-3-7-sonnet-latest', metadata={}, api_model_type='llm', provider_id='anthropic', type='model', provider_resource_id='anthropic/claude-3-7-sonnet-latest', model_type='llm'), Model(identifier='anthropic/voyage-3', metadata={'embedding_dimension': 1024.0, 'context_length': 32000.0}, api_model_type='embedding', provider_id='anthropic', type='model', provider_resource_id='anthropic/voyage-3', model_type='embedding'), Model(identifier='anthropic/voyage-3-lite', metadata={'embedding_dimension': 512.0, 'context_length': 32000.0}, api_model_type='embedding', provider_id='anthropic', type='model', provider_resource_id='anthropic/voyage-3-lite', model_type='embedding'), Model(identifier='anthropic/voyage-code-3', metadata={'embedding_dimension': 1024.0, 'context_length': 32000.0}, api_model_type='embedding', provider_id='anthropic', type='model', provider_resource_id='anthropic/voyage-code-3', model_type='embedding'), Model(identifier='fireworks/meta-llama/Llama-3.1-405B-Instruct-FP8', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-405b-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.1-70B-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-70b-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p1-8b-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.2-11B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-11b-vision-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-3b-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.2-90B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p2-90b-vision-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-v3p3-70b-instruct', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-4-Maverick-17B-128E-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama4-maverick-instruct-basic', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-4-Scout-17B-16E-Instruct', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama4-scout-instruct-basic', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-Guard-3-11B-Vision', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-guard-3-11b-vision', model_type='llm'), Model(identifier='fireworks/meta-llama/Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='fireworks', type='model', provider_resource_id='accounts/fireworks/models/llama-guard-3-8b', model_type='llm'), Model(identifier='fireworks/nomic-ai/nomic-embed-text-v1.5', metadata={'embedding_dimension': 768.0, 'context_length': 8192.0}, api_model_type='embedding', provider_id='fireworks', type='model', provider_resource_id='nomic-ai/nomic-embed-text-v1.5', model_type='embedding'), Model(identifier='gemini/gemini-1.5-flash', metadata={}, api_model_type='llm', provider_id='gemini', type='model', provider_resource_id='gemini/gemini-1.5-flash', model_type='llm'), Model(identifier='gemini/gemini-1.5-pro', metadata={}, api_model_type='llm', provider_id='gemini', type='model', provider_resource_id='gemini/gemini-1.5-pro', model_type='llm'), Model(identifier='gemini/gemini-2.0-flash', metadata={}, api_model_type='llm', provider_id='gemini', type='model', provider_resource_id='gemini/gemini-2.0-flash', model_type='llm'), Model(identifier='gemini/gemini-2.5-flash', metadata={}, api_model_type='llm', provider_id='gemini', type='model', provider_resource_id='gemini/gemini-2.5-flash', model_type='llm'), Model(identifier='gemini/gemini-2.5-pro', metadata={}, api_model_type='llm', provider_id='gemini', type='model', provider_resource_id='gemini/gemini-2.5-pro', model_type='llm'), Model(identifier='gemini/text-embedding-004', metadata={'embedding_dimension': 768.0, 'context_length': 2048.0}, api_model_type='embedding', provider_id='gemini', type='model', provider_resource_id='gemini/text-embedding-004', model_type='embedding'), Model(identifier='groq/llama-3.1-8b-instant', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-3.1-8b-instant', model_type='llm'), Model(identifier='groq/llama-3.2-3b-preview', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-3.2-3b-preview', model_type='llm'), Model(identifier='groq/llama-3.3-70b-versatile', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-3.3-70b-versatile', model_type='llm'), Model(identifier='groq/llama-4-maverick-17b-128e-instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-4-maverick-17b-128e-instruct', model_type='llm'), Model(identifier='groq/llama-4-scout-17b-16e-instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-4-scout-17b-16e-instruct', model_type='llm'), Model(identifier='groq/llama3-70b-8192', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama3-70b-8192', model_type='llm'), Model(identifier='groq/llama3-8b-8192', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama3-8b-8192', model_type='llm'), Model(identifier='groq/meta-llama/Llama-3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama3-70b-8192', model_type='llm'), Model(identifier='groq/meta-llama/Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama3-8b-8192', model_type='llm'), Model(identifier='groq/meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-3.2-3b-preview', model_type='llm'), Model(identifier='groq/meta-llama/Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-3.3-70b-versatile', model_type='llm'), Model(identifier='groq/meta-llama/Llama-4-Maverick-17B-128E-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-4-maverick-17b-128e-instruct', model_type='llm'), Model(identifier='groq/meta-llama/Llama-4-Scout-17B-16E-Instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/llama-4-scout-17b-16e-instruct', model_type='llm'), Model(identifier='groq/meta-llama/llama-4-maverick-17b-128e-instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/meta-llama/llama-4-maverick-17b-128e-instruct', model_type='llm'), Model(identifier='groq/meta-llama/llama-4-scout-17b-16e-instruct', metadata={}, api_model_type='llm', provider_id='groq', type='model', provider_resource_id='groq/meta-llama/llama-4-scout-17b-16e-instruct', model_type='llm'), Model(identifier='openai/chatgpt-4o-latest', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='openai/chatgpt-4o-latest', model_type='llm'), Model(identifier='openai/gpt-3.5-turbo', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-3.5-turbo', model_type='llm'), Model(identifier='openai/gpt-3.5-turbo-0125', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-3.5-turbo-0125', model_type='llm'), Model(identifier='openai/gpt-3.5-turbo-instruct', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-3.5-turbo-instruct', model_type='llm'), Model(identifier='openai/gpt-4', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4', model_type='llm'), Model(identifier='openai/gpt-4-turbo', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4-turbo', model_type='llm'), Model(identifier='openai/gpt-4o', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='openai/gpt-4o', model_type='llm'), Model(identifier='openai/gpt-4o-2024-08-06', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4o-2024-08-06', model_type='llm'), Model(identifier='openai/gpt-4o-audio-preview', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4o-audio-preview', model_type='llm'), Model(identifier='openai/gpt-4o-mini', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='openai/gpt-4o-mini', model_type='llm'), Model(identifier='openai/o1', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='o1', model_type='llm'), Model(identifier='openai/o1-mini', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='o1-mini', model_type='llm'), Model(identifier='openai/o3-mini', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='o3-mini', model_type='llm'), Model(identifier='openai/o4-mini', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='o4-mini', model_type='llm'), Model(identifier='openai/text-embedding-3-large', metadata={'embedding_dimension': 3072.0, 'context_length': 8192.0}, api_model_type='embedding', provider_id='openai', type='model', provider_resource_id='openai/text-embedding-3-large', model_type='embedding'), Model(identifier='openai/text-embedding-3-small', metadata={'embedding_dimension': 1536.0, 'context_length': 8192.0}, api_model_type='embedding', provider_id='openai', type='model', provider_resource_id='openai/text-embedding-3-small', model_type='embedding'), Model(identifier='sambanova/Llama-3.2-11B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-3.2-11B-Vision-Instruct', model_type='llm'), Model(identifier='sambanova/Llama-3.2-90B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-3.2-90B-Vision-Instruct', model_type='llm'), Model(identifier='sambanova/Llama-4-Maverick-17B-128E-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-4-Maverick-17B-128E-Instruct', model_type='llm'), Model(identifier='sambanova/Llama-4-Scout-17B-16E-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-4-Scout-17B-16E-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-3.1-405B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.1-405B-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.1-8B-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-3.2-1B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.2-1B-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.2-3B-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.3-70B-Instruct', model_type='llm'), Model(identifier='sambanova/Meta-Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-Guard-3-8B', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.1-405B-Instruct-FP8', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.1-405B-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.1-8B-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.2-11B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-3.2-11B-Vision-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.2-1B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.2-1B-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.2-3B-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.2-90B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-3.2-90B-Vision-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-3.3-70B-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-4-Maverick-17B-128E-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-4-Maverick-17B-128E-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-4-Scout-17B-16E-Instruct', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Llama-4-Scout-17B-16E-Instruct', model_type='llm'), Model(identifier='sambanova/meta-llama/Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='sambanova', type='model', provider_resource_id='sambanova/Meta-Llama-Guard-3-8B', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.1-405B-Instruct-FP8', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.1-70B-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-11B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-3B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-3B-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-3B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-90B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.3-70B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-3.3-70B-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-3.3-70B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-4-Maverick-17B-128E-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', model_type='llm'), Model(identifier='together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', model_type='llm'), Model(identifier='together/meta-llama/Llama-4-Scout-17B-16E-Instruct', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-4-Scout-17B-16E-Instruct', model_type='llm'), Model(identifier='together/meta-llama/Llama-Guard-3-11B-Vision', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-Guard-3-11B-Vision-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-Guard-3-11B-Vision-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Llama-Guard-3-11B-Vision-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-Guard-3-8B', model_type='llm'), Model(identifier='together/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', model_type='llm'), Model(identifier='together/meta-llama/Meta-Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='together', type='model', provider_resource_id='meta-llama/Meta-Llama-Guard-3-8B', model_type='llm'), Model(identifier='togethercomputer/m2-bert-80M-32k-retrieval', metadata={'embedding_dimension': 768.0, 'context_length': 32768.0}, api_model_type='embedding', provider_id='together', type='model', provider_resource_id='togethercomputer/m2-bert-80M-32k-retrieval', model_type='embedding'), Model(identifier='togethercomputer/m2-bert-80M-8k-retrieval', metadata={'embedding_dimension': 768.0, 'context_length': 8192.0}, api_model_type='embedding', provider_id='together', type='model', provider_resource_id='togethercomputer/m2-bert-80M-8k-retrieval', model_type='embedding'), Model(identifier='ollama/llama3.2:3b', metadata={}, api_model_type='llm', provider_id='ollama', type='model', provider_resource_id='llama3.2:3b', model_type='llm')]\n"
     ]
    }
   ],
   "source": [
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "models = client.models.list()\n",
    "print(models)\n",
    "model = \"ollama/llama3.2:3b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector DB: VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='toy_faiss_db', provider_id='faiss', type='vector_db', provider_resource_id='toy_faiss_db', owner={'principal': '', 'attributes': {}})\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "vector_db = client.vector_dbs.register(\n",
    "    vector_db_id=\"toy_faiss_db\",           # your chosen identifier\n",
    "    provider_id=\"faiss\",                   # builtâ€‘in inâ€‘memory FAISS\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",     # or pick another from client.models.list()\n",
    ")\n",
    "\n",
    "print(\"Created vector DB:\", vector_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents ingested.\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.types import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        document_id=\"dog1\",\n",
    "        content=\"Bella breed is a cavalier king.\",  # (also fixed typo from 'bread')\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={}\n",
    "    ),\n",
    "    Document(\n",
    "        document_id=\"dog2\",\n",
    "        content=\"Dora breed is a pug.\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={}\n",
    "    )\n",
    "]\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=docs,\n",
    "    vector_db_id=\"toy_faiss_db\",\n",
    "    chunk_size_in_tokens=128\n",
    ")\n",
    "\n",
    "print(\"Documents ingested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai\n",
      "fireworks\n",
      "together\n",
      "ollama\n",
      "anthropic\n",
      "gemini\n",
      "groq\n",
      "sambanova\n",
      "vllm\n",
      "sentence-transformers\n",
      "faiss\n",
      "meta-reference-files\n",
      "llama-guard\n",
      "meta-reference\n",
      "meta-reference\n",
      "meta-reference\n",
      "huggingface\n",
      "localfs\n",
      "basic\n",
      "llm-as-judge\n",
      "braintrust\n",
      "brave-search\n",
      "tavily-search\n",
      "rag-runtime\n",
      "model-context-protocol\n"
     ]
    }
   ],
   "source": [
    "providers = client.providers.list()\n",
    "for p in providers:\n",
    "    print(p.provider_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/toolgroups \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#register the RAG Toolgroup\n",
    "client.toolgroups.register(\n",
    "    toolgroup_id=\"rag-dogs\",\n",
    "    provider_id=\"rag-runtime\",  # â† This is the right one for your setup\n",
    "    args={\"vector_db_ids\": [\"toy_faiss_db\"]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=rag-dogs \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent \n",
    "\n",
    "# Instantiate without agent_id parameter\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=\"ollama/llama3.2:3b\",\n",
    "    instructions=\"Always use retrieval tool to fetch info about dog breeds before answering.\",\n",
    "    tools=[\"rag-dogs\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/toolgroups \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ToolGroup(identifier='builtin::rag', provider_id='rag-runtime', type='tool_group', args=None, mcp_endpoint=None, provider_resource_id='builtin::rag'),\n",
       " ToolGroup(identifier='builtin::websearch', provider_id='tavily-search', type='tool_group', args=None, mcp_endpoint=None, provider_resource_id='builtin::websearch'),\n",
       " ToolGroup(identifier='rag-dogs', provider_id='rag-runtime', type='tool_group', args={'vector_db_ids': ['toy_faiss_db']}, mcp_endpoint=None, provider_resource_id='rag-dogs')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.toolgroups.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/7aa4e87d-a3c9-4414-81e4-b7a34272d253/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/7aa4e87d-a3c9-4414-81e4-b7a34272d253/session/1b78cefa-d17a-4f41-8715-d52f1e420d23/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bella is a Cavalier King Charles Spaniel.\n"
     ]
    }
   ],
   "source": [
    "session_id = agent.create_session(session_name=\"dog-chat\")\n",
    "response = agent.create_turn(\n",
    "    session_id=session_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Which breed is Bella?\"}],\n",
    "     toolgroups=[{\n",
    "        \"toolgroup_id\": \"rag-dogs\",\n",
    "        \"name\": \"rag-dogs\",  # required!\n",
    "        \"args\": {\"vector_db_ids\": [\"toy_faiss_db\"]}\n",
    "    }],\n",
    "    stream=False,\n",
    ")\n",
    "print(\"Assistant:\", response.output_message.content)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/7aa4e87d-a3c9-4414-81e4-b7a34272d253/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/7aa4e87d-a3c9-4414-81e4-b7a34272d253/session/38d32432-2d3a-430a-967f-8a88027e51da/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The Dora breed is a pug.\n"
     ]
    }
   ],
   "source": [
    "session_id = agent.create_session(session_name=\"dog-chat\")\n",
    "response = agent.create_turn(\n",
    "    session_id=session_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Which breed is Dora?\"}],\n",
    "     toolgroups=[{\n",
    "        \"toolgroup_id\": \"rag-dogs\",\n",
    "        \"name\": \"rag-dogs\",  # required!\n",
    "        \"args\": {\"vector_db_ids\": [\"toy_faiss_db\"]}\n",
    "    }],\n",
    "    stream=False,\n",
    ")\n",
    "print(\"Assistant:\", response.output_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Tool: knowledge_search\n",
      "ğŸ“¥ Arguments: {'query': 'Dora breed'}\n",
      "ğŸªª Call ID: 2d62270f-5040-41ed-bd92-dddc38859b97\n"
     ]
    }
   ],
   "source": [
    "for step in response.steps:\n",
    "    if hasattr(step, \"api_model_response\") and hasattr(step.api_model_response, \"tool_calls\"):\n",
    "        for tool_call in step.api_model_response.tool_calls:\n",
    "            print(\"ğŸ”§ Tool:\", tool_call.tool_name)\n",
    "            print(\"ğŸ“¥ Arguments:\", tool_call.arguments)\n",
    "            print(\"ğŸªª Call ID:\", tool_call.call_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/toolgroups \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  RAG Toolgroup: builtin::rag\n",
      "   Args: None\n",
      "ğŸ§  RAG Toolgroup: rag-dogs\n",
      "   Args: {'vector_db_ids': ['toy_faiss_db']}\n"
     ]
    }
   ],
   "source": [
    "toolgroups = client.toolgroups.list()\n",
    "\n",
    "for tg in toolgroups:\n",
    "    if tg.provider_id == \"rag-runtime\":\n",
    "        print(f\"ğŸ§  RAG Toolgroup: {tg.identifier}\")\n",
    "        print(f\"   Args: {tg.args}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/toolgroups \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.types.tool_group import McpEndpoint \n",
    "\n",
    "from llama_stack_client.types.tool_group import McpEndpoint\n",
    "\n",
    "client.toolgroups.register(\n",
    "    toolgroup_id=\"mcp::my1\",\n",
    "    provider_id=\"model-context-protocol\",\n",
    "    mcp_endpoint=McpEndpoint(uri=\"http://localhost:8421/sse\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uv run uvicorn mcp_server:app --host 0.0.0.0 --port 8421 --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/tools in 0.433570 seconds\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/tools in 0.954515 seconds\n"
     ]
    },
    {
     "ename": "APITimeoutError",
     "evalue": "Request timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/_base_client.py:970\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPITimeoutError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agentMCP = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama/llama3.2:3b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAlways retrieve info using RAG before answering.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcp::my1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/lib/agents/agent.py:197\u001b[39m, in \u001b[36mAgent.__init__\u001b[39m\u001b[34m(self, client, agent_config, client_tools, tool_parser, model, instructions, tools, tool_config, sampling_params, max_infer_iters, input_shields, output_shields, response_format, enable_session_persistence, extra_headers)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mself\u001b[39m.builtin_tools = {}\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m.extra_headers = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/lib/agents/agent.py:207\u001b[39m, in \u001b[36mAgent.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_config[\u001b[33m\"\u001b[39m\u001b[33mtoolgroups\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    206\u001b[39m     toolgroup_id = tg \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m tg.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoolgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoolgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m         \u001b[38;5;28mself\u001b[39m.builtin_tools[tool.identifier] = tg.get(\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m, {}) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tg, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/resources/tools.py:73\u001b[39m, in \u001b[36mToolsResource.list\u001b[39m\u001b[34m(self, toolgroup_id, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist\u001b[39m(\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     50\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m     58\u001b[39m ) -> ToolListResponse:\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    List tools with optional tool group.\u001b[39;00m\n\u001b[32m     61\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/tools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoolgroup_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolgroup_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_list_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToolListParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataWrapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mToolListResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_unwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mToolListResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataWrapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mToolListResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1178\u001b[39m, in \u001b[36mSyncAPIClient.get\u001b[39m\u001b[34m(self, path, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1175\u001b[39m opts = FinalRequestOptions.construct(method=\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m, url=path, **options)\n\u001b[32m   1176\u001b[39m \u001b[38;5;66;03m# cast is required because mypy complains about returning Any even though\u001b[39;00m\n\u001b[32m   1177\u001b[39m \u001b[38;5;66;03m# it understands the type variables\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llama-stack/playground/.venv/lib/python3.12/site-packages/llama_stack_client/_base_client.py:988\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    985\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    987\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising timeout error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    990\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAPITimeoutError\u001b[39m: Request timed out."
     ]
    }
   ],
   "source": [
    "agentMCP = Agent(\n",
    "    client,\n",
    "    model=\"ollama/llama3.2:3b\",\n",
    "    instructions=\"Always retrieve info using RAG before answering.\",\n",
    "    tools=[\"mcp::my1\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
